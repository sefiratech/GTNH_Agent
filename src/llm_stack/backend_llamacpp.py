# llama.cpp-specific backend implementation
