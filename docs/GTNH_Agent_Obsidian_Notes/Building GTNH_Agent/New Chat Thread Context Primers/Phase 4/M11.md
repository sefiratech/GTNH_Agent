Here is an overview of the entire project for your context:

Phase 0:

M0 - environment_foundation

**Purpose:**

Lock in the actual environment & runtimes.

- Define:

- MC 1.7.10 + Forge 10.13.4.1614 + GTNH 2.8.1 run profile

- Decision: external bot client vs in-process Forge mod with IPC

- Hardware constraints for local LLMs

- **Dependencies:** None

- **Difficulty:** ⭐

- **Scalability notes:**

- Document this in a single config file / README; future changes (new model, new server host) should not touch code.

M1 - agent_architecture_spec

**Purpose:**  
Unify Mineflayer + Voyager insights into a **single architecture spec**.

- Extract from Mineflayer:
    
    - Bot lifecycle
        
    - World model
        
    - Pathfinding
        
    - Action abstraction
        
- Extract from Voyager:
    
    - Planner → Skill library → Execution loop
        
    - Reflection & learning
        
- **Dependencies:** `M0`
    
- **Difficulty:** ⭐⭐
    
- **Scalability notes:**
    
    - Produce one canonical architecture doc: diagrams + interfaces.
        
    - This is the contract everything else conforms to.


Phase 1

M2 - llm_stack_local

**Purpose:**  
Provide reusable interfaces around local models.

- Implement:
    
    - `PlannerModel`: high-level plan generation
        
    - `CodeModel`: skill/code generation
        
    - `CriticModel`: evaluation / refinement
        
- Unified tool schema:
    
    - Input: structured state / goal
        
    - Output: JSON plan / skill spec, no direct MC calls
        
- **Dependencies:** `M1`
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Centralize model loading & caching.
        
    - Make batch calls possible.
        
    - Log prompts/responses for replay.

M3 - world_semantics_gtnh

**Purpose:**  
Define GTNH tech + world understanding as **data + logic**.

- Data layer (config files):
    
    - Block categories (ores, machines, cables, etc.)
        
    - Item categories (plates, circuits, tools)
        
    - Tech states & prereqs (LV steam, MV, etc.)
        
- Logic layer (Python):
    
    - `infer_tech_state(inventory, machines)`
        
    - `suggest_next_targets(tech_state)`
        
    - `craftable_items(inventory, known_recipes)`
        
- **Dependencies:** `M1`
    
- **Difficulty:** ⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Keep recipes & categories in JSON/YAML, not code.
        
    - Cache derived graphs (like tech dependency DAGs).

M4 - virtue_lattice

**Purpose:**  
Encapsulate your Sefirot-based virtues as a reusable scoring layer.

- Define:
    
    - Virtue nodes: Efficiency, Safety, Sustainability, etc.
        
    - Configurable weights per context (e.g., early LV vs late HV)
        
- APIs:
    
    - `score_plan(plan, context) -> dict[virtue -> score]`
        
    - `compare_plans(plans, context) -> best_plan`
        
- **Dependencies:** `M3` (for context & environment semantics)
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Pure functions, stateless, easy to unit test.
        
    - Configurable weights → you can tune without code changes.

M5 - skill_registry

**Purpose:**  
Central place for skill definitions and metadata.

- Skill spec:
    
    - Name, parameters
        
    - Preconditions (what world/tech state is required)
        
    - Effects (changes in world/tech state)
        
    - Tags (e.g., mining, crafting, building)
        
- LLM interaction:
    
    - Planner only sees skill metadata, not raw code.
        
    - Skill implementations live as Python methods or small scripts.
        
- **Dependencies:** `M1`, `M3`
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Skills registered via decorators or config files.
        
    - Easy to version and deprecate skills over time.

Phase 2:

M6 - bot_core_1_7_10

**Purpose:**  
Provide a stable, testable “body” that can be used by any controller.

- Capabilities:
    
    - Connect/keepalive
        
    - World tracking (chunks, entities)
        
    - Navigation (A* or similar)
        
    - Actions:
        
        - Move, jump, break block, place block, use item, interact with tile entities
            
- API:
    
    - `observe() -> RawWorldSnapshot`
        
    - `execute_action(Action) -> Result`
        
- **Dependencies:** `M0`, `M1`
    
- **Difficulty:** ⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Keep logic modular: pathfinding, inventory, world tracking as submodules.
        
    - Limit unnecessary packet decoding; cache what you can.

M7 - observation_encoding

**Purpose:**  
Map `RawWorldSnapshot` from `M6` into semantic state used by LLMs & planners.

- Functions:
    
    - `encode_for_planner(raw_snapshot, tech_state) -> JSON`
        
    - `encode_for_critic(trace) -> JSON`
        
- Uses:
    
    - `M3` (semantics)
        
    - `M4` (virtues context)
        
- **Dependencies:** `M3`, `M6`
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Keep encodings compact. Summaries + key entities, not entire chunks.
        
    - Enforce stable schema to avoid breaking old skills.

Phase 3:

M8 - agent_loop_v1

**Purpose:**  
Implement the core loop: observe → plan → choose skills → act → evaluate.

- High-level algorithm:
    
    1. `state = observe()`
        
    2. `tech_state = infer_tech_state(state)`
        
    3. `plan = planner_model.call(state, tech_state, skill_registry, virtues)`
        
    4. Decompose plan into skill invocations
        
    5. Execute via `bot_core_1_7_10`
        
    6. Log result for learning (`M10`)
        
- Strict separation:
    
    - No direct packet calls here.
        
    - No GTNH-hardcoded weirdness here; that lives in `M3` and `M5`.
        
- **Dependencies:**
    
    - `M2` (LLM stack)
        
    - `M3` (world semantics)
        
    - `M4` (virtues)
        
    - `M5` (skills)
        
    - `M6` (bot core)
        
    - `M7` (observation encoding)
        
- **Difficulty:** ⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Design as a state machine with clear states (Idle, Planning, Executing, Recovering).
        
    - Rate-limit LLM calls, reuse plans until invalidated.

M9 - monitoring_and_tools

**Purpose:**  
Give you observability and a control surface before the system gaslights you.

- Features:
    
    - Structured logs (JSON)
        
    - Web or TUI dashboard:
        
        - World overview
            
        - Current plan & skills
            
        - Virtue scores
            
        - Tech state
            
    - Manual controls:
        
        - Pause, step, cancel plan, inspect memory
            
- **Dependencies:** `M8`
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Central logger used by all modules.
        
    - Minimal UI first; upgrade visuals later.


Phase 4:

M10 - skill_learning

**Purpose:**  
Voyager-style learning: derive new skills from experience and refine existing ones.

- Components:
    
    - Experience buffer:
        
        - `{state, goal, plan, actions, outcomes, virtue_scores}`
            
    - LLM-based synthesizer:
        
        - Turn repeated success traces into new skill definitions
            
    - Evaluator:
        
        - Compare new vs existing skills on:
            
            - Success rate
                
            - Cost (time, resources)
                
            - Virtue scores
                
- **Dependencies:** `M8` (loop), `M2` (LLMs), `M5` (skill registry), `M4` (virtue scoring)
    
- **Difficulty:** ⭐⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Learning should be offline or scheduled, not constant.
        
    - Skills versioned and can be rolled back if regressions appear.


M11 - gtnh_curriculum_and_specialization

**Purpose:**  
Turn the generic learning agent into a **GTNH-native progression engine**.

- Define:
    
    - Curricula per phase:
        
        - Early LV goals
            
        - Steam infra goals
            
        - MV automation goals
            
    - Long-horizon projects:
        
        - Stargate, high-tier reactors, etc.
            
- The curriculum is:
    
    - A sequence of target tech states
        
    - Each with:
        
        - Reward shaping (virtue weight tweaks)
            
        - Suggested skills to prioritize / learn
            
- **Dependencies:** `M3`, `M5`, `M8`, `M10`
    
- **Difficulty:** ⭐⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Curriculum is config, not code.
        
    - Multiple curricula can be swapped (e.g. “eco base”, “speedrun”, “aesthetic build”).


Shortcut View:
# **Phase P0 — Foundations**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M0**|environment_foundation|⭐|0.5–2 days|Lock runtime, modpack, IPC choice|
|**M1**|agent_architecture_spec|⭐⭐|2–4 days|Full architecture doc|

### **Phase P0 Total:**

**Difficulty Avg:** ⭐⭐  
**Time:** ~3–6 days

---

# **Phase P1 — Offline Core Pillars (No Minecraft)**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M2**|llm_stack_local|⭐⭐–⭐⭐⭐|3–7 days|Local models, prompt tooling|
|**M3**|world_semantics_gtnh|⭐⭐⭐⭐|7–14 days|Tech tree + ontology mapping|
|**M4**|virtue_lattice|⭐⭐–⭐⭐⭐|3–6 days|Scoring/weights system|
|**M5**|skill_registry|⭐⭐–⭐⭐⭐|3–6 days|Skill definitions, metadata|

### **Phase P1 Total:**

**Difficulty Avg:** ⭐⭐⭐  
**Time:** ~2–4 weeks

---

# **Phase P2 — Minecraft Integration Layer**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M6**|bot_core_1_7_10|⭐⭐⭐⭐|2–4 weeks|Pathfinding, inventory, world tracking|
|**M7**|observation_encoding|⭐⭐–⭐⭐⭐|3–7 days|Convert raw MC data → semantic state|

### **Phase P2 Total:**

**Difficulty Avg:** ⭐⭐⭐⭐  
**Time:** ~3–5 weeks

---

# **Phase P3 — Agent Orchestration & Tooling**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M8**|agent_loop_v1|⭐⭐⭐⭐|1–2 weeks|Full observe → plan → act|
|**M9**|monitoring_and_tools|⭐⭐–⭐⭐⭐|3–7 days|Logs, dashboards, step controls|

### **Phase P3 Total:**

**Difficulty Avg:** ⭐⭐⭐⭐  
**Time:** ~2–3 weeks

---

# **Phase P4 — Learning & Specialization**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M10**|skill_learning|⭐⭐⭐⭐⭐|2–4 weeks|Voyager-style skill synthesis|
|**M11**|gtnh_curriculum_and_specialization|⭐⭐⭐⭐⭐|multi-week ongoing|Long-horizon GTNH progression logic|

### **Phase P4 Total:**

**Difficulty Avg:** ⭐⭐⭐⭐⭐  
**Time:** ~4–8+ weeks (ongoing beyond initial build)

---

# **Grand Totals (First-Pass Implementation)**

|Phase|Difficulty Avg|Total Time|
|---|---|---|
|**P0**|⭐⭐|3–6 days|
|**P1**|⭐⭐⭐|2–4 weeks|
|**P2**|⭐⭐⭐⭐|3–5 weeks|
|**P3**|⭐⭐⭐⭐|2–3 weeks|
|**P4**|⭐⭐⭐⭐⭐|4–8+ weeks|

---

# **Overall Estimate**

**Minimum:** ~11 weeks  
**Expected:** ~14–18 weeks  
**Ambitious agent with learning + GTNH specialization:** ~20–30 weeks ongoing refinement


File Structure:

```
. # Repo root for GTNH_Agent project  
├── bootstrap_structure.py # Script to (re)create baseline repo layout / scaffolding  
├── config # All YAML/JSON configuration driving the agent’s behavior  
│ ├── curricula # High-level “what to learn / practice” configs (M11+)  
│ │ ├── aesthetic_megabase.yaml # Curriculum for building pretty, high-end megabase  
│ │ ├── default_speedrun.yaml # Curriculum for fast tech progression / baseline play  
│ │ └── eco_factory.yaml # Curriculum tuned for efficiency / low waste progression  
│ ├── env.yaml # M0: environment profiles (hardware, models, modpack, paths)  
│ ├── gtnh_blocks.generated.yaml # Autogenerated block metadata (from CSV/ingest scripts)  
│ ├── gtnh_blocks.yaml # Hand-tuned / merged GTNH block semantics  
│ ├── gtnh_items.generated.yaml # Autogenerated item metadata (from CSV/ingest scripts)  
│ ├── gtnh_items.yaml # Hand-tuned / merged GTNH item semantics  
│ ├── gtnh_recipes.agent.json # Recipes in agent-friendly format (condensed for planning)  
│ ├── gtnh_recipes.generated.json # Raw/generated recipe dump direct from ingestion tools  
│ ├── gtnh_recipes.json # Canonical GTNH recipe database for semantics layer  
│ ├── gtnh_tech_graph.yaml # Tech tree: tiers, unlocks, dependencies for GTNH progression  
│ ├── hardware.yaml # Hardware profiles / capabilities (GPU, RAM, CPU constraints)  
│ ├── llm_roles.yaml # Role configurations for planner/critic/scribe/error-model, etc  
│ ├── minecraft.yaml # Minecraft instance + world profiles (IPC ports, paths, seeds)  
│ ├── models.yaml # Model registry: which LLMs exist and how to address them  
│ ├── raw # Raw data dumps before normalization into GTNH configs  
│ │ ├── block.csv # Raw block listing exported from Tellme/NERD/NEI tooling  
│ │ ├── item.csv # Raw item listing exported from Tellme/NERD/NEI tooling  
│ │ ├── recipes.json # Raw recipe dump (uncompressed, noisy, pre-agent normalization)  
│ │ └── recipes_stacks.json # Recipe dump including stack sizes / quantities  
│ ├── skill_packs # Bundles of skills enabled for specific progression phases  
│ │ ├── lv_core.yaml # LV-tier core skill pack (what’s allowed/active in LV)  
│ │ └── steam_age.yaml # Steam Age skill pack (pre-LV core automation behaviors)  
│ ├── skills # YAML skill specs synced to Python skill implementations  
│ │ ├── basic_crafting.yaml # High-level crafting skill (generic recipe execution)  
│ │ ├── chop_tree.yaml # Tree chopping skill spec (coords, safety, replant hooks)  
│ │ ├── feed_coke_ovens.yaml # Keep coke ovens fueled with logs/coal, manage throughput  
│ │ ├── feed_steam_boiler.yaml # Maintain boiler fuel levels without exploding everything  
│ │ ├── maintain_coke_ovens.yaml # Higher-level coke oven maintenance (ash, outputs, uptime)  
│ │ ├── plant_sapling.yaml # Sapling placement skill for sustainable wood farms  
│ │ └── refill_water_tanks.yaml # Water tank / boiler refill skill spec  
│ ├── skills_candidates # Output dir for M10 learned/refined skills awaiting review  
│ ├── tools # Small config-oriented utilities  
│ │ ├── print_env.py # Debug helper to print resolved EnvProfile / config state  
│ │ └── validate_env.py # M0 validator: check env/hardware/models configs are sane  
│ └── virtues.yaml # Definition of virtue lattice, weights, constraints (M4)  
├── docs # Human-facing design docs & architecture notes  
│ ├── architecture.md # High-level system architecture across modules M0–M11  
│ ├── ipc_protocol_m6.md # IPC protocol spec for bot_core ↔ Minecraft (M6)  
│ ├── m6_bot_core_1_7_10.md # Design doc for 1.7.10 bot_core implementation  
│ └── phase1_integration.md # Phase 1 integration plan (M6–M9 wiring & runtime)  
├── .github # CI / repo automation config  
│ └── workflows  
│ └── ci.yml # GitHub Actions: linting/tests for pushes/PRs  
├── .gitignore # Git ignore list for build, logs, venvs, etc  
├── logs # Runtime logs; LLM calls, traces, debugging artifacts  
│ └── llm  
│ ├── 20251127T154508_30389_error_model_analyze_failure.json  
│ ├── 20251127T154508_30389_plan_code_plan.json  
│ ├── 20251127T154508_30389_scribe_summarize_trace.json  
│ ├── 20251127T154653_30421_plan_code_plan.json  
│ ├── 20251127T154929_30485_error_model_analyze_failure.json  
│ ├── 20251127T155054_30542_scribe_summarize_trace.json  
│ ├── 20251127T171641_54578_plan_code_plan.json  
│ ├── 20251127T172238_54966_plan_code_plan.json  
│ ├── 20251127T172404_55071_plan_code_plan.json  
│ ├── 20251127T181107_57121_plan_code_plan.json  
│ ├── 20251127T181432_57416_plan_code_plan.json  
│ ├── 20251127T202123_80866_error_model_analyze_failure.json  
│ ├── 20251127T202123_80866_plan_code_plan.json  
│ ├── 20251127T202123_80866_scribe_summarize_trace.json  
│ ├── 20251127T202334_80953_error_model_analyze_failure.json  
│ ├── 20251127T202334_80953_plan_code_plan.json  
│ ├── 20251127T202334_80953_scribe_summarize_trace.json  
│ ├── 20251127T223951_107281_error_model_analyze_failure.json  
│ ├── 20251127T223951_107281_plan_code_plan.json  
│ ├── 20251127T223951_107281_scribe_summarize_trace.json  
│ ├── 20251128T001446_136742_error_model_analyze_failure.json  
│ ├── 20251128T001446_136742_plan_code_plan.json  
│ ├── 20251128T001446_136742_scribe_summarize_trace.json  
│ ├── 20251128T003700_146815_error_model_analyze_failure.json  
│ ├── 20251128T003700_146815_plan_code_plan.json  
│ ├── 20251128T003700_146815_scribe_summarize_trace.json  
│ ├── 20251128T115106_24456_error_model_analyze_failure.json  
│ ├── 20251128T115106_24456_plan_code_plan.json  
│ ├── 20251128T115106_24456_scribe_summarize_trace.json  
│ ├── 20251128T143640_52852_error_model_analyze_failure.json  
│ ├── 20251128T143640_52852_plan_code_plan.json  
│ ├── 20251128T143640_52852_scribe_summarize_trace.json  
│ ├── 20251128T143816_52937_error_model_analyze_failure.json  
│ ├── 20251128T143816_52937_plan_code_plan.json  
│ ├── 20251128T143816_52937_scribe_summarize_trace.json  
│ ├── 20251128T153451_109564_error_model_analyze_failure.json  
│ ├── 20251128T153451_109564_plan_code_plan.json  
│ ├── 20251128T153451_109564_scribe_summarize_trace.json  
│ ├── 20251128T154112_109913_error_model_analyze_failure.json  
│ ├── 20251128T154112_109913_plan_code_plan.json  
│ ├── 20251128T154112_109913_scribe_summarize_trace.json  
│ ├── 20251128T154622_110074_error_model_analyze_failure.json  
│ ├── 20251128T154622_110074_plan_code_plan.json  
│ ├── 20251128T154622_110074_scribe_summarize_trace.json  
│ ├── 20251128T155834_129265_error_model_analyze_failure.json  
│ ├── 20251128T155834_129265_plan_code_plan.json  
│ ├── 20251128T155834_129265_scribe_summarize_trace.json  
│ ├── 20251128T160155_129475_error_model_analyze_failure.json  
│ ├── 20251128T160155_129475_plan_code_plan.json  
│ ├── 20251128T160155_129475_scribe_summarize_trace.json  
│ ├── 20251128T160633_142703_error_model_analyze_failure.json  
│ ├── 20251128T160633_142703_plan_code_plan.json  
│ ├── 20251128T160633_142703_scribe_summarize_trace.json  
│ ├── 20251128T162316_143085_error_model_analyze_failure.json  
│ ├── 20251128T162316_143085_plan_code_plan.json  
│ ├── 20251128T162316_143085_scribe_summarize_trace.json  
│ ├── 20251128T162956_163507_error_model_analyze_failure.json  
│ ├── 20251128T162956_163507_plan_code_plan.json  
│ ├── 20251128T162956_163507_scribe_summarize_trace.json  
│ ├── 20251128T164527_184688_error_model_analyze_failure.json  
│ ├── 20251128T164527_184688_plan_code_plan.json  
│ ├── 20251128T164527_184688_scribe_summarize_trace.json  
│ ├── 20251128T164725_184780_error_model_analyze_failure.json  
│ ├── 20251128T164725_184780_plan_code_plan.json  
│ ├── 20251128T164725_184780_scribe_summarize_trace.json  
│ ├── 20251128T164846_185052_error_model_analyze_failure.json  
│ ├── 20251128T164846_185052_plan_code_plan.json  
│ ├── 20251128T164846_185052_scribe_summarize_trace.json  
│ ├── 20251128T190228_210275_error_model_analyze_failure.json  
│ ├── 20251128T190228_210275_plan_code_plan.json  
│ ├── 20251128T190228_210275_scribe_summarize_trace.json  
│ ├── 20251128T190457_210363_error_model_analyze_failure.json  
│ ├── 20251128T190457_210363_plan_code_plan.json  
│ ├── 20251128T190457_210363_scribe_summarize_trace.json  
│ ├── 20251128T191608_225560_error_model_analyze_failure.json  
│ ├── 20251128T191608_225560_plan_code_plan.json  
│ ├── 20251128T191608_225560_scribe_summarize_trace.json  
│ ├── 20251128T192046_233385_error_model_analyze_failure.json  
│ ├── 20251128T192046_233385_plan_code_plan.json  
│ ├── 20251128T192046_233385_scribe_summarize_trace.json  
│ ├── 20251128T192251_233478_error_model_analyze_failure.json  
│ ├── 20251128T192251_233478_plan_code_plan.json  
│ ├── 20251128T192251_233478_scribe_summarize_trace.json  
│ ├── 20251128T204447_291001_error_model_analyze_failure.json  
│ ├── 20251128T204447_291001_plan_code_plan.json  
│ ├── 20251128T204501_291001_scribe_summarize_trace.json  
│ ├── 20251128T204732_291164_error_model_analyze_failure.json  
│ ├── 20251128T204732_291164_plan_code_plan.json  
│ ├── 20251128T204807_291164_scribe_summarize_trace.json  
│ ├── 20251128T212201_317178_error_model_analyze_failure.json  
│ ├── 20251128T212201_317178_plan_code_plan.json  
│ ├── 20251128T212258_317178_scribe_summarize_trace.json  
│ ├── 20251128T214908_353050_error_model_analyze_failure.json  
│ ├── 20251128T214908_353050_plan_code_plan.json  
│ ├── 20251128T215005_353050_scribe_summarize_trace.json  
│ ├── 20251128T215508_371511_error_model_analyze_failure.json  
│ ├── 20251128T215508_371511_plan_code_plan.json  
│ ├── 20251128T215605_371511_scribe_summarize_trace.json  
│ ├── 20251128T215741_371600_error_model_analyze_failure.json  
│ ├── 20251128T215741_371600_plan_code_plan.json  
│ ├── 20251128T215838_371600_scribe_summarize_trace.json  
│ ├── 20251129T120138_42538_error_model_analyze_failure.json  
│ ├── 20251129T120138_42538_plan_code_plan.json  
│ ├── 20251129T120235_42538_scribe_summarize_trace.json  
│ ├── 20251129T121735_52787_error_model_analyze_failure.json  
│ ├── 20251129T121735_52787_plan_code_plan.json  
│ ├── 20251129T121832_52787_scribe_summarize_trace.json  
│ ├── 20251129T122136_52946_error_model_analyze_failure.json  
│ ├── 20251129T122136_52946_plan_code_plan.json  
│ ├── 20251129T122233_52946_scribe_summarize_trace.json  
│ ├── 20251129T131642_83734_error_model_analyze_failure.json  
│ ├── 20251129T131642_83734_plan_code_plan.json  
│ ├── 20251129T131738_83734_scribe_summarize_trace.json  
│ ├── 20251129T131934_83828_error_model_analyze_failure.json  
│ ├── 20251129T131934_83828_plan_code_plan.json  
│ ├── 20251129T132032_83828_scribe_summarize_trace.json  
│ ├── 20251129T134009_114623_error_model_analyze_failure.json  
│ ├── 20251129T134009_114623_plan_code_plan.json  
│ ├── 20251129T134106_114623_scribe_summarize_trace.json  
│ ├── 20251129T135607_136708_error_model_analyze_failure.json  
│ ├── 20251129T135607_136708_plan_code_plan.json  
│ ├── 20251129T135703_136708_scribe_summarize_trace.json  
│ ├── 20251129T140350_168767_error_model_analyze_failure.json  
│ ├── 20251129T140350_168767_plan_code_plan.json  
│ ├── 20251129T140447_168767_scribe_summarize_trace.json  
│ ├── 20251129T145028_202536_error_model_analyze_failure.json  
│ ├── 20251129T145028_202536_plan_code_plan.json  
│ ├── 20251129T145124_202536_scribe_summarize_trace.json  
│ ├── 20251129T145252_202626_error_model_analyze_failure.json  
│ ├── 20251129T145252_202626_plan_code_plan.json  
│ └── 20251129T145349_202626_scribe_summarize_trace.json  
├── pyproject.toml # Project metadata: dependencies, package config, test settings  
├── .pytest_cache # Pytest’s cached run metadata (safe to delete, auto-regenerated)  
│ ├── CACHEDIR.TAG  
│ ├── .gitignore  
│ ├── README.md  
│ └── v  
│ └── cache  
│ ├── lastfailed # Record of last failing tests for quick reruns  
│ └── nodeids # Cached list of test node IDs  
├── .python-version # Pyenv version pin for local Python interpreter  
├── README.md # Top-level project description and usage notes  
├── scripts # One-off and maintenance CLI scripts  
│ ├── compact_recipes_for_agent.py # Compress/transform recipe DB into agent format  
│ ├── demo_offline_agent_step.py # Run a single offline agent step (LLM + loop smoke)  
│ ├── dev_shell.py # Developer REPL / convenience launch script  
│ ├── ingest_gtnh_semantics.py # Main ingest pipeline for GTNH semantics data  
│ ├── ingest_nerd_csv_semantics.py # Import NERD CSV dumps into semantics format  
│ ├── ingest_nerd_recipes.py # Recipe importer from NERD / NEI data sources  
│ ├── smoke_error_model.py # Quick smoke test for error_model behavior  
│ ├── smoke_llm_stack.py # Quick smoke test for full LLM stack pipeline  
│ └── smoke_scribe_model.py # Smoke test for scribe summarization model  
├── src # All runtime/source code for the agent system  
│ ├── agent # High-level agent orchestration (Phase 1 runtime shell)  
│ │ ├── bootstrap.py # Bootstrap agent wiring (env + runtime + loop)  
│ │ ├── experience.py # Glue for shaping + recording episodes (M8↔M10 bridge)  
│ │ ├── logging_config.py # Logging configuration setup for agent processes  
│ │ ├── loop.py # Agent loop orchestration wrapper over agent_loop module  
│ │ └── runtime_m6_m7.py # Runtime glue for bot_core + observation integration  
│ ├── agent_loop # M8: core agent loop (plan → act → observe → summarize)  
│ │ ├── **init**.py  
│ │ ├── loop.py # Implementation of AgentLoopV1 (offline/online variants)  
│ │ ├── schema.py # Dataclasses for plan, steps, transitions, episode state  
│ │ └── state.py # AgentLoop state machine / stepwise control logic  
│ ├── app # App-level entrypoints / wrappers  
│ │ ├── **init**.py  
│ │ └── runtime.py # High-level “run the agent” application harness  
│ ├── bot_core # M6: in-world movement, actions, IPC with Minecraft  
│ │ ├── actions.py # Concrete low-level actions (break/place/move/use/etc)  
│ │ ├── collision.py # Simple collision model / block solidity checks  
│ │ ├── core.py # Core bot control: ticks, dispatch, integration layer  
│ │ ├── **init**.py  
│ │ ├── nav  
│ │ │ ├── grid.py # Navigation grid abstraction for 3D world  
│ │ │ ├── **init**.py  
│ │ │ ├── mover.py # Stepwise movement primitives using pathfinder  
│ │ │ └── pathfinder.py # Pathfinding algorithms (A*, cost heuristics, etc)  
│ │ ├── net  
│ │ │ ├── client.py # IPC client to Forge mod / external runtime  
│ │ │ ├── external_client.py # Alternate client for external bot runtimes  
│ │ │ ├── **init**.py  
│ │ │ └── ipc.py # IPC protocol codec & socket handling  
│ │ ├── runtime.py # Bot runtime wrapper; ties nav + actions + net  
│ │ ├── snapshot.py # World snapshot type / capture helpers  
│ │ ├── testing  
│ │ │ └── fakes.py # BotCore fakes/mocks for tests  
│ │ ├── tracing.py # Bot-level tracing / debug hooks  
│ │ └── world_tracker.py # Local world state tracker for nearby chunks/blocks  
│ ├── cli  
│ │ └── phase1_offline.py # Offline CLI for running Phase 1 simulation episodes  
│ ├── curriculum # M11 curriculum engine implementation  
│ │ ├── engine.py # Curriculum scheduler / unit selection logic  
│ │ ├── **init**.py  
│ │ ├── loader.py # Load curricula YAML into in-memory models  
│ │ └── schema.py # Curriculum dataclasses / validation schema  
│ ├── env  
│ │ ├── **init**.py  
│ │ ├── loader.py # Load env.yaml/hardware.yaml/models into EnvProfile  
│ │ └── schema.py # EnvProfile and related config dataclasses  
│ ├── gtnh_agent.egg-info # Package metadata generated by build tooling  
│ │ ├── dependency_links.txt  
│ │ ├── PKG-INFO  
│ │ ├── requires.txt  
│ │ ├── SOURCES.txt  
│ │ └── top_level.txt  
│ ├── **init**.py # Package root marker for `src` layout  
│ ├── integration # Phase 1 integration glue & validators  
│ │ ├── adapters  
│ │ │ └── m0_env_to_world.py # Adapter from env configs → world/semantics view  
│ │ ├── episode_logging.py # Episode logging hook (bridge to M9 + logs/)  
│ │ ├── **init**.py  
│ │ ├── phase1_integration.py # Orchestration of M6–M9 in a single runtime  
│ │ ├── testing  
│ │ │ ├── fakes.py # Integration fakes (world, semantics, etc)  
│ │ │ └── **init**.py  
│ │ └── validators  
│ │ ├── **init**.py  
│ │ ├── planner_guardrails.py # Guardrails on plan structures / unsafe actions  
│ │ ├── semantics_snapshots.py # Semantic snapshot checks vs GTNH data  
│ │ ├── skill_integrity.py # Ensure skill YAML ↔ code implementations are aligned  
│ │ └── virtue_snapshots.py # Sanity snapshots for virtue scoring across runs  
│ ├── learning # M10: skill learning & experience subsystem  
│ │ ├── buffer.py # JSONL experience buffer (append/filter episodes)  
│ │ ├── evaluator.py # Compare baseline vs candidate skill performance  
│ │ ├── **init**.py  
│ │ ├── manager.py # SkillLearningManager orchestrating full learning cycles  
│ │ ├── schema.py # ExperienceEpisode / SkillCandidate / stats dataclasses  
│ │ └── synthesizer.py # LLM-based skill synthesizer using M2 CodeModel  
│ ├── llm_stack # M2: fully local LLM orchestration layer  
│ │ ├── backend_llamacpp.py # llama.cpp backend integration for local models  
│ │ ├── backend.py # Abstract backend interface for different runtimes  
│ │ ├── codegen.py # Code generation utilities (for skills, etc)  
│ │ ├── config.py # LLM stack config loader (models.yaml, roles, etc)  
│ │ ├── critic.py # Critic role (plan review, failure analysis)  
│ │ ├── error_model.py # Error model / failure analyzer (Reflexion-like)  
│ │ ├── **init**.py  
│ │ ├── json_utils.py # Strict JSON/structured output helpers  
│ │ ├── log_files.py # Helpers for writing/reading LLM log JSONs  
│ │ ├── plan_code.py # Planner implementation using the code model  
│ │ ├── planner.py # Planner orchestration & API  
│ │ ├── presets.py # Planner / critic presets (prompt+settings)  
│ │ ├── schema.py # LLM call/trace schema (requests, responses)  
│ │ ├── scribe.py # Scribe summarizer for traces and episodes  
│ │ └── stack.py # Top-level LLM stack object (wires planner/critic/scribe)  
│ ├── monitoring # M9: monitoring, event bus, TUI dashboards  
│ │ ├── bus.py # Event bus for monitoring events  
│ │ ├── controller.py # Monitoring controller coordinating tools + logger  
│ │ ├── dashboard_tui.py # Text UI dashboard for runtime status  
│ │ ├── events.py # Event type definitions for monitoring system  
│ │ ├── **init**.py  
│ │ ├── integration.py # Glue hooks from runtime/agent_loop into monitoring  
│ │ ├── llm_logging.py # Specialized logging of LLM interactions  
│ │ ├── logger.py # Monitoring logger with structured logs  
│ │ └── tools.py # Interactive / CLI tools over event bus  
│ ├── observation # M7: from world snapshots → planner/critic-friendly state  
│ │ ├── encoder.py # Encode world state into semantic representation  
│ │ ├── **init**.py  
│ │ ├── pipeline.py # Observation pipeline orchestration  
│ │ ├── schema.py # Observation data models / DTOs  
│ │ ├── testing.py # Observation test helpers  
│ │ └── trace_schema.py # PlanTrace / step schemas used across M7–M10  
│ ├── runtime # Phase bootstrap + runtime error handling  
│ │ ├── agent_runtime_main.py # Main entrypoint for running agent runtime  
│ │ ├── bootstrap_phases.py # Phase-based bootstrap (P0, P1, etc)  
│ │ ├── error_handling.py # Shared error-handling utilities  
│ │ ├── failure_mitigation.py # Higher-level recovery strategies around failures  
│ │ └── **init**.py  
│ ├── semantics # M3: GTNH world semantics & tech modeling  
│ │ ├── cache.py # Semantics cache layer (avoid recomputation)  
│ │ ├── categorize.py # Item/block categorization (ores, plates, circuits, etc)  
│ │ ├── crafting.py # Craftability checks, recipe reasoning helpers  
│ │ ├── ingest  
│ │ │ └── **init**.py # Ingest package marker for semantics  
│ │ ├── **init**.py  
│ │ ├── loader.py # Load GTNH data from config/gtnh_* files  
│ │ ├── schema.py # Types for items, blocks, recipes, tech_state  
│ │ └── tech_state.py # TechState implementation: active tier, unlocks, flags  
│ ├── skills # M5: runtime skill system  
│ │ ├── base # Concrete skill implementations matching config/skills  
│ │ │ ├── basic_crafting.py  
│ │ │ ├── chop_tree.py  
│ │ │ ├── feed_coke_ovens.py  
│ │ │ ├── feed_steam_boiler.py  
│ │ │ ├── **init**.py  
│ │ │ ├── maintain_coke_ovens.py  
│ │ │ ├── plant_sapling.py  
│ │ │ └── refill_water_tanks.py  
│ │ ├── **init**.py  
│ │ ├── loader.py # Load skill YAML & bind to Python implementations  
│ │ ├── packs.py # Skill pack resolution / activation logic  
│ │ ├── registry.py # Central SkillRegistry (lookup, metadata, activation)  
│ │ └── schema.py # SkillSpec / runtime signature types  
│ ├── spec # Abstract “specs” describing contracts between modules  
│ │ ├── agent_loop.py # Contract spec for AgentLoop behavior  
│ │ ├── bot_core.py # Contract between bot_core and rest of system  
│ │ ├── experience.py # Spec for experience & learning interfaces  
│ │ ├── **init**.py  
│ │ ├── llm.py # Spec for LLM stack roles and interfaces  
│ │ ├── skills.py # Spec for skill interface & expectations  
│ │ └── types.py # Shared type aliases and small protocol definitions  
│ ├── testing  
│ │ └── **init**.py # Testing utilities namespace  
│ └── virtues # M4: virtue lattice, scoring, constraints  
│ ├── explain.py # Explain virtue scores / decisions in human terms  
│ ├── features.py # Feature extraction feeding virtue metrics  
│ ├── **init**.py  
│ ├── lattice.py # Virtue lattice math / scoring engine  
│ ├── loader.py # Load virtues.yaml into runtime structures  
│ ├── metrics.py # Metric definitions used to evaluate plans/actions  
│ ├── sanity.py # Sanity checks for virtue config & outputs  
│ └── schema.py # Types for virtues, metrics, constraints  
├── tests # Pytest suite covering all modules/phases  
│ ├── conftest.py # Shared pytest fixtures and config  
│ ├── fakes # Reusable fake subsystems for tests  
│ │ ├── fake_bot_core.py # Fake bot_core for offline tests  
│ │ ├── fake_llm_stack.py # Fake LLM stack (no real model calls)  
│ │ ├── fake_runtime.py # Fake runtime harness for integration tests  
│ │ ├── fake_skills.py # Fake skills for unit/integration testing  
│ │ └── **init**.py  
│ ├── **init**.py  
│ ├── test_actions.py # Tests for bot_core.actions behaviors  
│ ├── test_agent_loop_stub.py # Minimal stub AgentLoop tests  
│ ├── test_agent_loop_v1.py # Full AgentLoopV1 tests  
│ ├── test_architecture_integration.py # Sanity for architecture invariants  
│ ├── test_bot_core_impl.py # Bot core implementation tests  
│ ├── test_env_loader.py # Env loader + config validation tests  
│ ├── test_error_model_with_fake_backend.py # Error-model behavior with fake backend  
│ ├── test_evaluator.py # Unit tests for M10 SkillEvaluator  
│ ├── test_experience_buffer.py # Roundtrip and filtering tests for ExperienceBuffer  
│ ├── test_failure_mitigation.py # Tests for runtime failure mitigation strategies  
│ ├── test_full_system_smoke.py # End-to-end smoke for major pipeline  
│ ├── test_llm_stack_fake_backend.py # LLM stack tests using fake backend  
│ ├── test_m6_observe_contract.py # Contract tests between bot_core & observation  
│ ├── test_monitoring_controller.py # Monitoring controller behavior tests  
│ ├── test_monitoring_dashboard_tui.py # TUI dashboard tests (layout & events)  
│ ├── test_monitoring_event_bus.py # Monitoring bus behavior & event routing  
│ ├── test_monitoring_logger.py # Monitoring logger tests  
│ ├── test_nav_pathfinder.py # Pathfinding algorithm tests  
│ ├── test_observation_critic_encoding.py # Encoding tests for critic input  
│ ├── test_observation_perf.py # Performance-focused observation tests  
│ ├── test_observation_pipeline.py # Observation pipeline integration tests  
│ ├── test_observation_planner_encoding.py # Planner input encoding tests  
│ ├── test_observation_worldstate_normalization.py # Worldstate normalization tests  
│ ├── test_p0_p1_env_bridge.py # P0→P1 bridge tests (env into world/runtime)  
│ ├── test_phase012_bootstrap.py # Bootstrap for early phases M0–M2  
│ ├── test_phase0_runtime.py # Phase 0 runtime behavior tests  
│ ├── test_phase1_breakglass_no_plans.py # Edge-case: no-plan scenarios handling  
│ ├── test_phase1_integration_offline.py # Phase 1 offline integration tests  
│ ├── test_runtime_integration.py # Runtime module integration tests  
│ ├── test_runtime_m6_m7_smoke.py # Smoke tests for runtime_m6_m7 wiring  
│ ├── test_scribe_model_with_fake_backend.py # Scribe behavior with fake LLM  
│ ├── test_semantics_caching_singleton.py # Semantics cache singleton behavior  
│ ├── test_semantics_categorization.py # Categorization tests for semantics  
│ ├── test_semantics_craftability.py # Craftability / recipe-use tests  
│ ├── test_semantics_tech_inference.py # Tech tier inference tests  
│ ├── test_semantics_tolerant_fallbacks.py # Fallback behavior for missing data  
│ ├── test_semantics_with_normalized_worldstate.py # Semantics with normalized state  
│ ├── test_skill_evaluator.py # M10 evaluator tests (virtue-aware comparison)  
│ ├── test_skill_learning_manager.py # End-to-end M10 learning cycle tests  
│ ├── test_skill_loader.py # Skill loader YAML↔code tests  
│ ├── test_skill_packs_integrity.py # Integrity checks for skill_packs config  
│ ├── test_skill_packs.py # Behavior tests for skill pack resolution  
│ ├── test_skill_registry.py # SkillRegistry behavior tests  
│ ├── test_synthesizer.py # M10 SkillSynthesizer tests with fake model  
│ ├── test_virtue_compare_plans.py # Virtue-based plan comparison tests  
│ ├── test_virtue_config_sanity.py # Sanity checks for virtue config file  
│ ├── test_virtue_hard_constraints.py # Tests for hard virtue constraints  
│ ├── test_virtue_lattice_basic.py # Core virtue lattice behavior tests  
│ └── test_world_tracker.py # World tracker behavior tests  
└── tools # Dev tools / demos separate from scripts/  
├── agent_demo.py # Interactive demo for agent planning/execution  
├── phase1_demo.py # Demo for Phase 1 integration run  
└── smoke_botcore.py # Standalone smoke test for bot_core behavior
```



## 1. Big Picture Snapshot

**Project:** `GTNH_Agent`  
**Goal:** Fully local GTNH automation agent with:

- World semantics (M3)
    
- Virtue lattice alignment (M4)
    
- Skill system (M5)
    
- Bot runtime (M6–M7)
    
- Agent loop (M8)
    
- Monitoring (M9)
    
- Skill learning (M10)
    
- Next: Curriculum engine (M11) to drive _what_ the agent practices and learns.
    

You are now at the point where the agent has:

- **Experience memory** (episodes)
    
- **Self-evaluation** (metrics + virtue-aware evaluator)
    
- **Self-improvement hooks** (SkillLearningManager, candidate skill artifacts)
    

M11’s job is to become the _teacher_: deciding which goals, skills, and tech-states to focus on and when.

---

## 2. Core Concepts & Definitions (Current State)

### 2.1 Environment & Semantics

- **Config core**
    
    - `config/env.yaml` + `hardware.yaml` + `models.yaml`  
        Define runtime profile, hardware limits, and model stack.
        
    - `config/minecraft.yaml`  
        Where/what world the agent thinks it’s in.
        
- **GTNH semantics (M3)**
    
    - `config/gtnh_*` + `scripts/ingest_*` + `src/semantics/*`
        
    - Encodes:
        
        - Items, blocks, and recipes
            
        - `TechState` tiers and unlocks (`tech_state.py`)
            
        - Craftability and categorization logic (`crafting.py`, `categorize.py`)
            

Semantics now serve as the ground truth for planning, virtue evaluation, and skill specialization.

---

### 2.2 Virtue Lattice (M4)

- **Config:** `config/virtues.yaml`
    
- **Code:** `src/virtues/{loader,metrics,lattice,sanity,schema}.py`
    

Key ideas:

- Plans and actions are evaluated not just on “does it work” but:
    
    - Safety
        
    - Efficiency
        
    - Resource stewardship
        
    - Whatever other virtues you defined in `virtues.yaml`
        
- `virtues.metrics` extracts plan/world metrics.
    
- `virtues.lattice` uses those metrics + config to produce per-virtue scores.
    
- This is now wired into M10’s **SkillEvaluator**, so skills are ranked not only by success rate, but by virtue alignment.
    

---

### 2.3 Skill System (M5)

- **Specs:** `config/skills/*.yaml`
    
- **Packs:** `config/skill_packs/*.yaml`
    
- **Code:** `src/skills/*`
    

Pieces:

- **SkillSpec** schema (`skills/schema.py`)
    
- **SkillRegistry** (`skills/registry.py`)
    
    - Keeps skill metadata, registration, and lookup.
        
- **Base skills** (`skills/base/*.py`)
    
    - `basic_crafting`, `chop_tree`, `feed_coke_ovens`, `feed_steam_boiler`, etc.
        
- **Skill packs** (`skills/packs.py` + `config/skill_packs/`)
    
    - Define which skills are active in Steam Age, LV, etc.
        

M10 now adds a _parallel_ skill source: `config/skills_candidates/`  
These are _not_ auto-loaded. They wait for human/curriculum approval.

---

### 2.4 Bot Core & Observation (M6–M7)

- **Bot core:** `src/bot_core/*`
    
    - IPC, pathfinding, movement, actions, world tracking.
        
- **Observation pipeline:** `src/observation/*`
    
    - World snapshots → normalized features → planner/critic/LLM-friendly state.
        
- **Trace schema:** `observation/trace_schema.py`
    
    - Defines `PlanTrace`, steps, actions, results, metadata.
        

These are the “eyes, ears, and hands” feeding into M8 and eventually into M10’s episode recording.

---

### 2.5 Agent Loop & Monitoring (M8–M9)

- **Agent loop:** `src/agent_loop/{loop,schema,state}.py`
    
    - Implements the core cycle:
        
        1. Observe
            
        2. Plan (LLM planner)
            
        3. Act (bot_core)
            
        4. Summarize (scribe)
            
    - Works offline and integrates with runtime shells (`src/agent/*`, `src/runtime/*`).
        
- **Monitoring (M9):** `src/monitoring/*`
    
    - Event bus, controller, TUI, structured logging.
        
    - LLM logs written under `logs/llm/` (plan, error_model, scribe traces).
        
    - Hooks into loop/runtime so you can watch the agent thrash in real time.
        

Monitoring is also the “tap” where M10 can grab traces / state for learning.

---

## 3. M10 – Skill Learning (Just Finished)

This is the new chunk of machinery you just wired in.

### 3.1 Experience Schema

- **File:** `src/learning/schema.py`
    

Defines:

- `ExperienceEpisode`
    
    - `id` – unique identifier
        
    - `goal` – planner’s goal text
        
    - `tech_state` – `TechState` snapshot
        
    - `trace` – `PlanTrace` from M7/M8
        
    - `virtue_scores` – M4 scoring for this episode
        
    - `success` – boolean outcome
        
    - `metadata` – timestamps, environment profile, tags
        
- `SkillPerformanceStats`
    
    - Aggregated stats for a skill:
        
        - uses
            
        - success_rate
            
        - avg_time
            
        - avg_resource_cost
            
        - avg_virtue_scores
            
- `SkillCandidate`
    
    - Represents a _proposed_ or _refined_ skill:
        
        - `id`
            
        - `base_skill_name` (if refining)
            
        - `spec_yaml`
            
        - `impl_code`
            
        - `rationale`
            
        - `status` (“proposed”, etc)
            
        - `metrics_before` / `metrics_after`
            
        - `extra` (prompt metadata, tags, etc)
            

This schema is the bridge between **M8 episodes** and **M10 learning**.

---

### 3.2 Experience Buffer

- **File:** `src/learning/buffer.py`
    
- **Storage:** `JSONL` file, one episode per line.
    

Features:

- `append(episode)`
    
    - Takes an `ExperienceEpisode`, serializes tech_state + trace via the injected converters.
        
- `load_all()` / `load_all_raw()`
    
    - Streams episodes back out as objects or raw dicts.
        
- Filtering helpers:
    
    - `iter_by_success(success: bool)`
        
    - `iter_by_skill_usage(skill_name)`
        
    - `iter_by_tech_tier(tier)`
        
    - `count()`
        

This is the **offline memory** backing M10 & M11. Episodes are no longer just logs; they’re machine-readable experience.

---

### 3.3 Synthesizer (LLM Skill Synthesis)

- **File:** `src/learning/synthesizer.py`
    
- **Uses:** M2 **CodeModel** (`spec.llm.CodeModel`)
    

Responsibilities:

- Input:
    
    - Cluster of similar successful `ExperienceEpisode`s
        
    - Optional `target_skill_name` (for refinement)
        
    - `candidate_id`
        
- Internal:
    
    - Builds a structured payload containing:
        
        - Trace summaries (goals, steps, skill tags, success flags)
            
        - Tech state (tier, unlocks)
            
        - Virtue scores
            
        - Output schema description (spec_yaml, impl_code, rationale)
            
- Output:
    
    - `SkillCandidate` with:
        
        - `spec_yaml` – YAML spec for the new or refined skill
            
        - `impl_code` – Python stub/skeleton
            
        - `rationale` – explanation of design choices
            
        - `status="proposed"`
            
        - `extra["synthesizer_prompt"]` containing provenance info
            

This is the “researcher” proposing new skill abstractions from repeated success.

---

### 3.4 Evaluator (Baseline vs Candidate)

- **File:** `src/learning/evaluator.py`
    
- Uses M4 via:
    
    - `load_virtue_config`
        
    - `extract_plan_metrics` (module-level shim)
        
    - `score_plan` (module-level shim)
        

Responsibilities:

1. **Aggregate stats**
    
    - `aggregate_skill_stats(episodes, skill_name, context_id, skill_metadata, semantics_db)`
        
    - Computes `SkillPerformanceStats`:
        
        - `uses`
            
        - `success_rate`
            
        - `avg_time`
            
        - `avg_resource_cost`
            
        - averaged per-virtue scores.
            
2. **Compare stats**
    
    - `compare_stats(baseline, candidate, ...)` returns:
        
        - `recommendation`: `"promote_candidate" | "keep_baseline" | "reject_candidate"`
            
        - `reasons`: tags like:
            
            - `candidate_insufficient_data`
                
            - `candidate_success_rate_too_low`
                
            - `candidate_faster`
                
            - `candidate_cheaper`
                
            - `candidate_better_<virtue_name>`
                
        - Serialized baseline & candidate stats.
            

Evaluator is the **performance review board**: does the new thing beat the old thing, and is it ethically less stupid?

---

### 3.5 SkillLearningManager (Orchestrator)

- **File:** `src/learning/manager.py`
    

Responsibilities:

- Read episodes from `ExperienceBuffer`
    
- Filter/cluster by:
    
    - `goal_substring`
        
    - `skill_name`
        
    - `tech_tier`
        
    - `success_only`
        
- Call `SkillSynthesizer` to create a `SkillCandidate`
    
- Use `SkillEvaluator` to compute baseline stats (and, later, A/B stats)
    
- Persist candidate artifacts:
    
    - `config/skills_candidates/<id>.yaml` – spec
        
    - `config/skills_candidates/<id>.py` – impl stub
        
    - `config/skills_candidates/<id>.meta.json` – candidate + evaluation
        
- Return structured result summary for tooling/M11.
    

Core API:
```python
result = manager.run_learning_cycle_for_goal(
    goal_substring="maintain coke ovens",
    target_skill_name="maintain_coke_ovens",  # or None for new skill
    context_id="steam_age",
    tech_tier="steam_age",
    success_only=True,
    min_episodes=5,
)


```

Result contents:

- `candidate`: `SkillCandidate`
    
- `baseline_stats`: `SkillPerformanceStats | None`
    
- `candidate_stats`: currently same as baseline until A/B exists
    
- `evaluation`: recommendation dict or `None`
    
- `episodes_used`: list of episode IDs
    

This is the **offline R&D supervisor**: it knows how to go from logs → proposal → evaluation → artifact.

---

### 3.6 Tests & Validation

You actually got all this under tests:

- **Buffer:**
    
    - Roundtrip & filters via `test_experience_buffer.py`
        
- **Synthesizer:**
    
    - `test_synthesizer.py` with `FakeCodeModel`
        
- **Evaluator:**
    
    - `test_evaluator.py` & `test_skill_evaluator.py` with fake virtues
        
- **Manager:**
    
    - End-to-end learning cycle + file creation via `test_skill_learning_manager.py`
        

So M10 isn’t theoretical; it’s wired, tested, and your pytest run isn’t complaining (for once).

---

## 4. Design Shifts & New Capabilities

You introduced some meaningful upgrades:

1. **Episodes became first-class citizens**
    
    - Previously: traces & logs were passive artifacts.
        
    - Now: `ExperienceEpisode` objects are structured and replayable, with virtue scores and tech state attached.
        
2. **Learning is now grounded in GTNH semantics**
    
    - Skill evaluation uses real tech tiers, recipe semantics, and virtues instead of abstract “success / failure.”
        
3. **Skill evolution is decoupled from runtime**
    
    - M10 does _not_ auto-deploy new skills.
        
    - It writes candidates to `config/skills_candidates/` for:
        
        - human review
            
        - curriculum-driven activation (M11)
            
        - future auto-promotion logic
            
4. **Virtue-aware performance evaluation**
    
    - “Better” now means:
        
        - at least comparable success rate
            
        - improved time and/or resource cost
            
        - and/or better virtue scores
            
    - You’ve built a path for alignment to actually constrain optimization instead of just being a nice word in a doc.
        

---

## 5. M11 – Curriculum Engine: What Needs to Happen

M11 sits on top of all this and decides **what the agent practices and learns next**.

### 5.1 Files & Modules Involved

You already have the skeleton:

- **Config:**
    
    - `config/curricula/*.yaml`
        
        - `aesthetic_megabase.yaml`
            
        - `default_speedrun.yaml`
            
        - `eco_factory.yaml`
            
- **Code:**
    
    - `src/curriculum/schema.py`
        
    - `src/curriculum/loader.py`
        
    - `src/curriculum/engine.py`
        

M11’s job is to make these _actually_ drive:

- episode generation
    
- skill improvement
    
- candidate review & activation
    

---

### 5.2 What a Curriculum Should Control

At a high level, each curriculum should answer:

1. **What goals to pursue now?**
    
    - Example:
        
        - “Maintain steam boiler uptime above X.”
            
        - “Automate coke production for Y hours.”
            
        - “Reach LV tech while minimizing coal usage.”
            
2. **What skills to exercise / stress-test?**
    
    - Use `SkillRegistry` + skill packs:
        
        - e.g., Steam Age → `steam_age.yaml`
            
        - LV Core → `lv_core.yaml`
            
    - Optionally bias towards:
        
        - under-trained skills
            
        - newly added candidate skills (for A/B)
            
3. **Which tech_state & environment preconditions?**
    
    - Require certain tier:
        
        - `tech_state.active == "steam_age"` or `"lv"`
            
    - Check unlocks:
        
        - “Has railcraft boilers”
            
        - “Has coke ovens”
            
        - etc.
            
4. **What success criteria and metrics matter?**
    
    - Curriculum-specific virtue weighting:
        
        - Eco-focused: resource efficiency, sustainability
            
        - Speedrun: time-to-tech-tier more heavily weighted
            
        - Aesthetic: maybe reward symmetry / low-chaos building patterns later
            
5. **What learning rules to apply?**
    
    - When to:
        
        - trigger a SkillLearningManager cycle
            
        - consider a candidate for promotion
            
        - retire or demote a baseline skill in a curriculum context
            

---

### 5.3 How M11 Should Integrate with Existing Modules

M11 sits in the middle of:

- `agent_loop` (M8): actually runs episodes.
    
- `learning` (M10): learns from episodes.
    
- `skills` (M5): provides executable behaviors.
    
- `semantics` + `virtues` (M3 + M4): context & evaluation.
    

**Rough flow:**

1. **Pick curriculum unit**
    
    - `curriculum.engine.select_unit(env_profile, tech_state, history)`
        
2. **Turn it into a goal / task config**
    
    - Generate:
        
        - goal string
            
        - constraints (tech tier, allowed skills, virtue weighting overrides, etc.)
            
3. **Run episodes under that unit**
    
    - Call into `agent_loop` via runtime / integration.
        
    - Ensure:
        
        - appropriate skill pack is activated
            
        - environment is correct
            
4. **Record episodes**
    
    - `src/agent/experience.py` + `integration/episode_logging.py` should:
        
        - convert M8 traces into `ExperienceEpisode`
            
        - append to `ExperienceBuffer`
            
5. **Trigger learning**
    
    - Based on curriculum rules:
        
        - e.g., after N successful episodes on a given goal, schedule:
```python
manager.run_learning_cycle_for_goal(
    goal_substring="maintain coke ovens",
    target_skill_name="maintain_coke_ovens",
    context_id="steam_age",
    tech_tier="steam_age",
)

```

6. **Update curriculum state

- Track:
    
    - which skills have candidate upgrades
        
    - which candidates await review
        
    - which tech milestones are satisfied
        
- Provide a machine-readable “progress state” for later modules.

---

## 6. Immediate Next Steps for M11

Here’s the “do this next” list you probably wanted without asking:

### 6.1 Lock Down Curriculum Schema

In `src/curriculum/schema.py`, define something like:

- `Curriculum`
    
- `CurriculumUnit`
    
- `UnitGoal`
    
- `UnitConstraints`
    
- `UnitLearningRules`
    

Make sure you can express:

- “Run N episodes with this goal substring”
    
- “Use these skill packs”
    
- “Trigger learning cycle for this skill when conditions met”
    
- “Stop when tech_state or metrics satisfy these conditions”
    

Update YAML examples in `config/curricula/*.yaml` to match.

---

### 6.2 Implement Loader Fully

In `src/curriculum/loader.py`:

- Load all curricula from `config/curricula/*.yaml`
    
- Validate against `schema.py`
    
- Provide:
    
    - `load_curriculum(name: str) -> Curriculum`
        
    - `list_curricula() -> List[str]`
        
    - Optional: simple integrity checks (skills referenced actually exist).
        

---

### 6.3 Build Curriculum Engine

In `src/curriculum/engine.py`:

Implement:

- `CurriculumEngine`
    
    - Tracks:
        
        - active curriculum
            
        - current unit
            
        - unit progress (episodes run, success ratio, etc.)
            
    - Provides:
        
        - `select_next_goal(tech_state, history) -> GoalConfig`
            
        - `on_episode_complete(episode: ExperienceEpisode) -> None`
            
        - `maybe_trigger_learning(manager: SkillLearningManager)`
            

You want the engine to be the _single point_ that decides:

- when to gather episodes
    
- when to call `SkillLearningManager`
    
- how to tag `context_id` and `goal_substring` per curriculum unit.
    

---

### 6.4 Wire into Runtime / Agent Loop

Update:

- `src/agent/loop.py` / `src/app/runtime.py` / `src/runtime/agent_runtime_main.py` (where appropriate) to:
    
    - Initialize:
        
        - `CurriculumEngine`
            
        - `ExperienceBuffer`
            
        - `SkillLearningManager`
            
    - For each episode:
        
        1. Ask curriculum: `goal = engine.select_next_goal(...)`
            
        2. Run episode via `agent_loop` with that goal.
            
        3. Convert trace → `ExperienceEpisode`
            
        4. Append to `ExperienceBuffer`
            
        5. Call `engine.on_episode_complete(episode)`
            
        6. Let engine decide if/when to call `SkillLearningManager`.
            

The idea is: **M11 decides, M8 acts, M10 learns.**

---

### 6.5 Add Curriculum-Focused Tests

Under `tests/`:

- `test_curriculum_loader.py`
    
    - YAML → schema validity, reference integrity.
        
- `test_curriculum_engine_basic.py`
    
    - Engine picks units sensibly.
        
    - Tracks unit completion.
        
- `test_curriculum_m10_integration.py`
    
    - Use fakes for:
        
        - `ExperienceBuffer`
            
        - `SkillLearningManager`
            
        - `AgentLoop` stub
            
    - Run a small synthetic “curriculum” and assert:
        
        - goals produced match spec
            
        - learning cycles called when expected
            
        - candidates written to `skills_candidates/` with appropriate IDs/tagging.
            

---

## 7. What You Have Now

By the end of M10 (and heading into M11), this is true:

- The agent has:
    
    - **Semantics**: GTNH item/block/recipe/tech state.
        
    - **Virtues**: value structure for evaluating plans.
        
    - **Skills**: executable behaviors with YAML+Python specs.
        
    - **Loop**: plan → act → observe → summarize.
        
    - **Monitoring**: events, logs, and a TUI.
        
    - **Experience**: episodes as structured dataclasses.
        
    - **Learning stack**: synthesizer, evaluator, manager, and buffer.
        
- M11 now becomes:
    
    - The **curriculum director**:
        
        - Defining training goals and projects.
            
        - Scheduling learning.
            
        - Managing which candidate skills even get a shot at promotion.
            

You’ve basically built a small research lab inside a Minecraft bot. The next step is giving it a syllabus instead of letting it just vibe with coke ovens forever.