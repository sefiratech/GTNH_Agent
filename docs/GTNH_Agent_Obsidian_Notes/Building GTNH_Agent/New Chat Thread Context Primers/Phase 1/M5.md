Here is an overview of the entire project for your context:

Phase 0:

M0 - environment_foundation

**Purpose:**

Lock in the actual environment & runtimes.

- Define:

- MC 1.7.10 + Forge 10.13.4.1614 + GTNH 2.8.1 run profile

- Decision: external bot client vs in-process Forge mod with IPC

- Hardware constraints for local LLMs

- **Dependencies:** None

- **Difficulty:** ⭐

- **Scalability notes:**

- Document this in a single config file / README; future changes (new model, new server host) should not touch code.

M1 - agent_architecture_spec

**Purpose:**  
Unify Mineflayer + Voyager insights into a **single architecture spec**.

- Extract from Mineflayer:
    
    - Bot lifecycle
        
    - World model
        
    - Pathfinding
        
    - Action abstraction
        
- Extract from Voyager:
    
    - Planner → Skill library → Execution loop
        
    - Reflection & learning
        
- **Dependencies:** `M0`
    
- **Difficulty:** ⭐⭐
    
- **Scalability notes:**
    
    - Produce one canonical architecture doc: diagrams + interfaces.
        
    - This is the contract everything else conforms to.


Phase 1

M2 - llm_stack_local

**Purpose:**  
Provide reusable interfaces around local models.

- Implement:
    
    - `PlannerModel`: high-level plan generation
        
    - `CodeModel`: skill/code generation
        
    - `CriticModel`: evaluation / refinement
        
- Unified tool schema:
    
    - Input: structured state / goal
        
    - Output: JSON plan / skill spec, no direct MC calls
        
- **Dependencies:** `M1`
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Centralize model loading & caching.
        
    - Make batch calls possible.
        
    - Log prompts/responses for replay.

M3 - world_semantics_gtnh

**Purpose:**  
Define GTNH tech + world understanding as **data + logic**.

- Data layer (config files):
    
    - Block categories (ores, machines, cables, etc.)
        
    - Item categories (plates, circuits, tools)
        
    - Tech states & prereqs (LV steam, MV, etc.)
        
- Logic layer (Python):
    
    - `infer_tech_state(inventory, machines)`
        
    - `suggest_next_targets(tech_state)`
        
    - `craftable_items(inventory, known_recipes)`
        
- **Dependencies:** `M1`
    
- **Difficulty:** ⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Keep recipes & categories in JSON/YAML, not code.
        
    - Cache derived graphs (like tech dependency DAGs).

M4 - virtue_lattice

**Purpose:**  
Encapsulate your Sefirot-based virtues as a reusable scoring layer.

- Define:
    
    - Virtue nodes: Efficiency, Safety, Sustainability, etc.
        
    - Configurable weights per context (e.g., early LV vs late HV)
        
- APIs:
    
    - `score_plan(plan, context) -> dict[virtue -> score]`
        
    - `compare_plans(plans, context) -> best_plan`
        
- **Dependencies:** `M3` (for context & environment semantics)
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Pure functions, stateless, easy to unit test.
        
    - Configurable weights → you can tune without code changes.

M5 - skill_registry

**Purpose:**  
Central place for skill definitions and metadata.

- Skill spec:
    
    - Name, parameters
        
    - Preconditions (what world/tech state is required)
        
    - Effects (changes in world/tech state)
        
    - Tags (e.g., mining, crafting, building)
        
- LLM interaction:
    
    - Planner only sees skill metadata, not raw code.
        
    - Skill implementations live as Python methods or small scripts.
        
- **Dependencies:** `M1`, `M3`
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Skills registered via decorators or config files.
        
    - Easy to version and deprecate skills over time.

Phase 2:

M6 - bot_core_1_7_10

**Purpose:**  
Provide a stable, testable “body” that can be used by any controller.

- Capabilities:
    
    - Connect/keepalive
        
    - World tracking (chunks, entities)
        
    - Navigation (A* or similar)
        
    - Actions:
        
        - Move, jump, break block, place block, use item, interact with tile entities
            
- API:
    
    - `observe() -> RawWorldSnapshot`
        
    - `execute_action(Action) -> Result`
        
- **Dependencies:** `M0`, `M1`
    
- **Difficulty:** ⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Keep logic modular: pathfinding, inventory, world tracking as submodules.
        
    - Limit unnecessary packet decoding; cache what you can.

M7 - observation_encoding

**Purpose:**  
Map `RawWorldSnapshot` from `M6` into semantic state used by LLMs & planners.

- Functions:
    
    - `encode_for_planner(raw_snapshot, tech_state) -> JSON`
        
    - `encode_for_critic(trace) -> JSON`
        
- Uses:
    
    - `M3` (semantics)
        
    - `M4` (virtues context)
        
- **Dependencies:** `M3`, `M6`
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Keep encodings compact. Summaries + key entities, not entire chunks.
        
    - Enforce stable schema to avoid breaking old skills.

Phase 3:

M8 - agent_loop_v1

**Purpose:**  
Implement the core loop: observe → plan → choose skills → act → evaluate.

- High-level algorithm:
    
    1. `state = observe()`
        
    2. `tech_state = infer_tech_state(state)`
        
    3. `plan = planner_model.call(state, tech_state, skill_registry, virtues)`
        
    4. Decompose plan into skill invocations
        
    5. Execute via `bot_core_1_7_10`
        
    6. Log result for learning (`M10`)
        
- Strict separation:
    
    - No direct packet calls here.
        
    - No GTNH-hardcoded weirdness here; that lives in `M3` and `M5`.
        
- **Dependencies:**
    
    - `M2` (LLM stack)
        
    - `M3` (world semantics)
        
    - `M4` (virtues)
        
    - `M5` (skills)
        
    - `M6` (bot core)
        
    - `M7` (observation encoding)
        
- **Difficulty:** ⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Design as a state machine with clear states (Idle, Planning, Executing, Recovering).
        
    - Rate-limit LLM calls, reuse plans until invalidated.

M9 - monitoring_and_tools

**Purpose:**  
Give you observability and a control surface before the system gaslights you.

- Features:
    
    - Structured logs (JSON)
        
    - Web or TUI dashboard:
        
        - World overview
            
        - Current plan & skills
            
        - Virtue scores
            
        - Tech state
            
    - Manual controls:
        
        - Pause, step, cancel plan, inspect memory
            
- **Dependencies:** `M8`
    
- **Difficulty:** ⭐⭐–⭐⭐⭐
    
- **Scalability/perf:**
    
    - Central logger used by all modules.
        
    - Minimal UI first; upgrade visuals later.


Phase 4:

M10 - skill_learning

**Purpose:**  
Voyager-style learning: derive new skills from experience and refine existing ones.

- Components:
    
    - Experience buffer:
        
        - `{state, goal, plan, actions, outcomes, virtue_scores}`
            
    - LLM-based synthesizer:
        
        - Turn repeated success traces into new skill definitions
            
    - Evaluator:
        
        - Compare new vs existing skills on:
            
            - Success rate
                
            - Cost (time, resources)
                
            - Virtue scores
                
- **Dependencies:** `M8` (loop), `M2` (LLMs), `M5` (skill registry), `M4` (virtue scoring)
    
- **Difficulty:** ⭐⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Learning should be offline or scheduled, not constant.
        
    - Skills versioned and can be rolled back if regressions appear.


M11 - gtnh_curriculum_and_specialization

**Purpose:**  
Turn the generic learning agent into a **GTNH-native progression engine**.

- Define:
    
    - Curricula per phase:
        
        - Early LV goals
            
        - Steam infra goals
            
        - MV automation goals
            
    - Long-horizon projects:
        
        - Stargate, high-tier reactors, etc.
            
- The curriculum is:
    
    - A sequence of target tech states
        
    - Each with:
        
        - Reward shaping (virtue weight tweaks)
            
        - Suggested skills to prioritize / learn
            
- **Dependencies:** `M3`, `M5`, `M8`, `M10`
    
- **Difficulty:** ⭐⭐⭐⭐⭐
    
- **Scalability/perf:**
    
    - Curriculum is config, not code.
        
    - Multiple curricula can be swapped (e.g. “eco base”, “speedrun”, “aesthetic build”).


Shortcut View:
# **Phase P0 — Foundations**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M0**|environment_foundation|⭐|0.5–2 days|Lock runtime, modpack, IPC choice|
|**M1**|agent_architecture_spec|⭐⭐|2–4 days|Full architecture doc|

### **Phase P0 Total:**

**Difficulty Avg:** ⭐⭐  
**Time:** ~3–6 days

---

# **Phase P1 — Offline Core Pillars (No Minecraft)**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M2**|llm_stack_local|⭐⭐–⭐⭐⭐|3–7 days|Local models, prompt tooling|
|**M3**|world_semantics_gtnh|⭐⭐⭐⭐|7–14 days|Tech tree + ontology mapping|
|**M4**|virtue_lattice|⭐⭐–⭐⭐⭐|3–6 days|Scoring/weights system|
|**M5**|skill_registry|⭐⭐–⭐⭐⭐|3–6 days|Skill definitions, metadata|

### **Phase P1 Total:**

**Difficulty Avg:** ⭐⭐⭐  
**Time:** ~2–4 weeks

---

# **Phase P2 — Minecraft Integration Layer**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M6**|bot_core_1_7_10|⭐⭐⭐⭐|2–4 weeks|Pathfinding, inventory, world tracking|
|**M7**|observation_encoding|⭐⭐–⭐⭐⭐|3–7 days|Convert raw MC data → semantic state|

### **Phase P2 Total:**

**Difficulty Avg:** ⭐⭐⭐⭐  
**Time:** ~3–5 weeks

---

# **Phase P3 — Agent Orchestration & Tooling**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M8**|agent_loop_v1|⭐⭐⭐⭐|1–2 weeks|Full observe → plan → act|
|**M9**|monitoring_and_tools|⭐⭐–⭐⭐⭐|3–7 days|Logs, dashboards, step controls|

### **Phase P3 Total:**

**Difficulty Avg:** ⭐⭐⭐⭐  
**Time:** ~2–3 weeks

---

# **Phase P4 — Learning & Specialization**

|Module|Name|Difficulty|Est. Time|Notes|
|---|---|---|---|---|
|**M10**|skill_learning|⭐⭐⭐⭐⭐|2–4 weeks|Voyager-style skill synthesis|
|**M11**|gtnh_curriculum_and_specialization|⭐⭐⭐⭐⭐|multi-week ongoing|Long-horizon GTNH progression logic|

### **Phase P4 Total:**

**Difficulty Avg:** ⭐⭐⭐⭐⭐  
**Time:** ~4–8+ weeks (ongoing beyond initial build)

---

# **Grand Totals (First-Pass Implementation)**

|Phase|Difficulty Avg|Total Time|
|---|---|---|
|**P0**|⭐⭐|3–6 days|
|**P1**|⭐⭐⭐|2–4 weeks|
|**P2**|⭐⭐⭐⭐|3–5 weeks|
|**P3**|⭐⭐⭐⭐|2–3 weeks|
|**P4**|⭐⭐⭐⭐⭐|4–8+ weeks|

---

File Structure:
```
GTNH_Agent/                                  # Project root: full GTNH Agent codebase
├── bootstrap_structure.py                   # One-shot scaffolder that originally created this layout
├── config/                                  # All declarative config; no logic, just data
│   ├── curricula/                           # High-level progression presets (what the agent "wants" long-term)
│   │   ├── aesthetic_megabase.yaml          # Curriculum: prioritize pretty, large bases & builds
│   │   ├── default_speedrun.yaml            # Curriculum: optimize for fast tech progression
│   │   └── eco_factory.yaml                 # Curriculum: prioritize efficiency/sustainability-style goals
│   ├── env.yaml                             # Env profiles: dev/prod, active model profile, bot_mode, etc.
│   ├── gtnh_blocks.generated.yaml           # Auto-ingested block semantics from Nerd/NEI dumps (do not hand-edit)
│   ├── gtnh_blocks.yaml                     # Hand-authored block overrides / manual tags for important blocks
│   ├── gtnh_items.generated.yaml            # Auto-ingested item semantics (huge but structured)
│   ├── gtnh_items.yaml                      # Hand-authored item overrides / special cases
│   ├── gtnh_recipes.agent.json              # Compressed, agent-ready recipe set (~215MB); primary recipe source
│   ├── gtnh_recipes.generated.json          # Raw-ish generated recipe dump (~400MB); kept as backup/reference
│   ├── gtnh_recipes.json                    # Manual / future overrides for specific recipes (currently minimal/empty)
│   ├── gtnh_tech_graph.yaml                 # Tech progression DAG: stone_age → steam → LV → MV etc + requirements
│   ├── hardware.yaml                        # Host hardware profiles (GPU, threads, n_gpu_layers, etc.)
│   ├── llm_roles.yaml                       # System prompts / roles for planner, critic, scribe, error model, etc.
│   ├── minecraft.yaml                       # Minecraft runtime config: server info, world, IPC/mod settings
│   ├── models.yaml                          # Logical model profiles: which backend, which GGUF, context length, etc.
│   ├── raw/                                 # Raw dumps from NEI / Nerd; ingestion inputs, not used directly by agent
│   │   ├── block.csv                        # Block registry dump from NEI (name/id/mod/class/etc.)
│   │   ├── item.csv                         # Item registry dump from NEI
│   │   ├── recipes.json                     # Absurd NEI recipe dump (pre-processed into agent/generated JSONs)
│   │   └── recipes_stacks.json              # Stack metadata used to resolve NEI’s weird itemSlug format
│   ├── skills/                              # YAML skill specifications (what the planner sees)
│   │   ├── chop_tree.yaml                   # Spec for basic tree-chopping skill
│   │   ├── feed_coke_ovens.yaml             # Spec for keeping coke ovens fed
│   │   └── plant_sapling.yaml               # Spec for planting saplings post-chop
│   ├── skills_candidates/                   # Staging folder for new skills auto-synthesized or WIP (currently empty)
│   ├── tools/                               # Tiny CLI helpers for config/env sanity
│   │   ├── print_env.py                     # Print resolved EnvProfile for debugging (what profile actually resolves to)
│   │   └── validate_env.py                  # Schema & consistency checks for env/models/hardware YAML
│   └── virtues.yaml                         # Virtue lattice definition: names, weights, dimensions per context
├── docs/
│   └── architecture.md                      # Human-readable architecture spec for the whole M0–M11 system
├── .github/
│   └── workflows/
│       └── ci.yml                           # CI config: lint/tests on push/PR
├── .gitignore                               # Ignore venvs, caches, logs & other junk
├── logs/
│   └── llm/                                 # All LLM call traces (planner, critic, scribe, error model)
│       ├── 20251127T154508_30389_error_model_analyze_failure.json   # ErrorModel run: failure-analysis payload & result
│       ├── 20251127T154508_30389_plan_code_plan.json                # Planner/codegen run log (inputs/outputs/timing)
│       ├── 20251127T154508_30389_scribe_summarize_trace.json        # Scribe summarization / trace compression log
│       ├── 20251127T154653_30421_plan_code_plan.json                # More plan/code runs (benchmarking / debugging)
│       ├── 20251127T154929_30485_error_model_analyze_failure.json   # ErrorModel diagnosis event
│       ├── 20251127T155054_30542_scribe_summarize_trace.json        # Scribe summarization output
│       ├── 20251127T171641_54578_plan_code_plan.json                # Etc.; each triplet: plan / scribe / error_model pass
│       ├── 20251127T172238_54966_plan_code_plan.json
│       ├── 20251127T172404_55071_plan_code_plan.json
│       ├── 20251127T181107_57121_plan_code_plan.json
│       ├── 20251127T181432_57416_plan_code_plan.json
│       ├── 20251127T202123_80866_error_model_analyze_failure.json
│       ├── 20251127T202123_80866_plan_code_plan.json
│       ├── 20251127T202123_80866_scribe_summarize_trace.json
│       ├── 20251127T202334_80953_error_model_analyze_failure.json
│       ├── 20251127T202334_80953_plan_code_plan.json
│       ├── 20251127T202334_80953_scribe_summarize_trace.json
│       ├── 20251127T223951_107281_error_model_analyze_failure.json
│       ├── 20251127T223951_107281_plan_code_plan.json
│       ├── 20251127T223951_107281_scribe_summarize_trace.json
│       ├── 20251128T001446_136742_error_model_analyze_failure.json
│       ├── 20251128T001446_136742_plan_code_plan.json
│       ├── 20251128T001446_136742_scribe_summarize_trace.json
│       ├── 20251128T003700_146815_error_model_analyze_failure.json
│       ├── 20251128T003700_146815_plan_code_plan.json
│       ├── 20251128T003700_146815_scribe_summarize_trace.json
│       ├── 20251128T115106_24456_error_model_analyze_failure.json
│       ├── 20251128T115106_24456_plan_code_plan.json
│       └── 20251128T115106_24456_scribe_summarize_trace.json
├── pyproject.toml                            # Project metadata, dependencies, tooling (pytest, ruff, etc.)
├── .pytest_cache/                            # Pytest internals; auto-generated junk
│   ├── CACHEDIR.TAG                          # Marker to indicate this is a cache dir
│   ├── .gitignore                            # Ensure cache content is not committed
│   ├── README.md                             # Pytest cache info
│   └── v/
│       └── cache/
│           ├── lastfailed                    # Which tests failed last run
│           └── nodeids                       # Collected test IDs
├── .python-version                           # Pyenv version pin (Python 3.12.x)
├── README.md                                 # Top-level description, setup, quickstart, status
├── scripts/                                  # Manual entrypoints, dev tools, ingestion & smoke tests
│   ├── compact_recipes_for_agent.py          # Compress big recipe dump → gtnh_recipes.agent.json
│   ├── demo_offline_agent_step.py            # Run a single offline agent step using fake world/LLM
│   ├── dev_shell.py                          # Interactive dev REPL: quick stack/semantics poking
│   ├── ingest_gtnh_semantics.py              # Orchestrator: runs the various ingestion passes into generated configs
│   ├── ingest_nerd_csv_semantics.py          # Convert Nerd/NEI block/item CSVs → gtnh_{blocks,items}.generated.yaml
│   ├── ingest_nerd_recipes.py                # Convert NEI recipe dumps → standardized recipe JSON format(s)
│   ├── smoke_error_model.py                  # Quick harness to exercise ErrorModel end-to-end
│   ├── smoke_llm_stack.py                    # Smoke & perf test for LLMStack / PlanCodeModel (tok/s, stability)
│   └── smoke_scribe_model.py                 # Smoke test for Scribe summarization / compression
├── src/                                      # Actual library code (this is the package)
│   ├── agent_loop/                           # M8: Agent orchestration core (observe → plan → act → evaluate)
│   │   ├── __init__.py                       # Package marker; may export main loop API
│   │   ├── loop.py                           # Main agent loop logic / state machine
│   │   ├── schema.py                         # Dataclasses/schemas for loop input/output structures
│   │   └── state.py                          # AgentState representation across timesteps/episodes
│   ├── app/
│   │   ├── __init__.py                       # Package marker
│   │   └── runtime.py                        # Top-level wiring: load env, init LLMStack, semantics, bot_core, monitoring
│   ├── bot_core/                             # M6: Bot body & Minecraft protocol-facing logic
│   │   ├── actions.py                        # Primitive actions (move, break, use, interact, etc.)
│   │   ├── core.py                           # Bot control loop; dispatches actions & tracks high-level state
│   │   ├── __init__.py                       # Package marker; may expose BotCore interface
│   │   ├── nav/                              # Navigation-related logic
│   │   │   ├── grid.py                       # Voxel/grid representation for pathfinding
│   │   │   ├── __init__.py                   # Package marker
│   │   │   ├── mover.py                      # Low-level movement executor following paths
│   │   │   └── pathfinder.py                 # A*/cost heuristics & navigation algorithms
│   │   ├── net/                              # Networking / IPC layer
│   │   │   ├── client.py                     # Minecraft client / protocol bridge to server or Forge mod
│   │   │   ├── __init__.py                   # Package marker
│   │   │   └── ipc.py                        # IPC abstraction between Python and in-game integration
│   │   ├── snapshot.py                       # Build raw world snapshots for observation encoder
│   │   └── world_tracker.py                  # Tracks world state over time (chunks, entities, block deltas)
│   ├── curriculum/                           # M11-adjacent: curriculum engine implementation
│   │   ├── engine.py                         # Selects next tasks/goals from curricula based on state
│   │   ├── __init__.py                       # Package marker
│   │   ├── loader.py                         # Loads curriculum YAMLs into in-memory structures
│   │   └── schema.py                         # Dataclasses for curriculum definitions
│   ├── env/                                  # M0: environment config loading & schema
│   │   ├── __init__.py                       # Package marker
│   │   ├── loader.py                         # Reads env.yaml + models.yaml + hardware.yaml → EnvProfile
│   │   └── schema.py                         # EnvProfile & related config types
│   ├── gtnh_agent.egg-info/                  # Packaging metadata generated by `pip install -e .`
│   │   ├── dependency_links.txt              # Setuptools-generated dependency links
│   │   ├── PKG-INFO                          # Package metadata (name, version, description)
│   │   ├── requires.txt                      # Installed dependencies list
│   │   ├── SOURCES.txt                       # Files included in the distribution
│   │   └── top_level.txt                     # Top-level package name(s)
│   ├── __init__.py                           # Makes `src` a package; can export root-level symbols later
│   ├── learning/                             # M10: skill learning & experience-based improvements
│   │   ├── buffer.py                         # Experience buffer / replay storage
│   │   ├── evaluator.py                      # Evaluate learned skills/strategies against metrics
│   │   ├── __init__.py                       # Package marker
│   │   ├── manager.py                        # Orchestrates learning phases, triggers training, updates registry
│   │   ├── schema.py                         # Schemas for experiences, metrics, training configs
│   │   └── synthesizer.py                    # Generates synthetic tasks/traces from curricula or logs
│   ├── llm_stack/                            # M2: Local LLM plumbing & orchestration
│   │   ├── backend_llamacpp.py               # llama.cpp backend wrapper (local GGUF w/ GPU offload)
│   │   ├── backend.py                        # Abstract LLMBackend protocol/interface
│   │   ├── codegen.py                        # Turn plans/specs into code (skills, helpers)
│   │   ├── config.py                         # ModelConfig, ModelProfile etc. for LLMStack
│   │   ├── critic.py                         # Critic model wrapper: score plans/actions/traces
│   │   ├── error_model.py                    # ErrorModel: analyze failures, suggest fixes / retries
│   │   ├── __init__.py                       # Exposes LLMStack & key helpers
│   │   ├── json_utils.py                     # Robust JSON parsing/coercion from messy LLM output
│   │   ├── log_files.py                      # Structured logging helpers for LLM calls → logs/llm
│   │   ├── plan_code.py                      # PlanCodeModel: plan + code generation pipeline
│   │   ├── planner.py                        # Planning-only model wrapper (structured plans, no code)
│   │   ├── presets.py                        # Prompt/parameter presets for specific roles & modes
│   │   ├── schema.py                         # Types for prompts, responses, configs
│   │   ├── scribe.py                         # Scribe model: summarization & trace compression
│   │   └── stack.py                          # High-level LLMStack orchestrator composing all sub-models
│   ├── monitoring/                           # M9: Observability & controls
│   │   ├── bus.py                            # Event bus for logging/monitoring events
│   │   ├── controller.py                     # Central controller: subscriptions, routing, wiring
│   │   ├── dashboard_tui.py                  # Terminal UI dashboard for live stats & debugging
│   │   ├── events.py                         # Event type definitions (LLM calls, actions, errors, etc.)
│   │   ├── __init__.py                       # Package marker
│   │   └── logger.py                         # Logging bridge: write to files/console/etc.
│   ├── observation/                          # M7: Encoding raw snapshots to LLM-ready observations
│   │   ├── encoder.py                        # Build normalized WorldState from BotCore snapshots
│   │   ├── __init__.py                       # Package marker
│   │   ├── schema.py                         # Observation & related schema definitions
│   │   └── trace_schema.py                   # Episode trace schemas (for learning & debugging)
│   ├── semantics/                            # M3: GTNH semantics, tech graph, and craftability logic
│   │   ├── cache.py                          # Singleton helpers so SemanticsDB/TechGraph only init once
│   │   ├── categorize.py                     # Helpers to map items/blocks to semantic categories
│   │   ├── crafting.py                       # `craftable_items` & inventory/machine helpers
│   │   ├── ingest/                           # Python package marker for ingest-related helpers (future)
│   │   │   └── __init__.py                   # Placeholder for ingestion-specific helpers
│   │   ├── __init__.py                       # Exposes semantics public API (db, helpers)
│   │   ├── loader.py                         # Loads & merges blocks/items/recipes configs into SemanticsDB
│   │   ├── schema.py                         # BlockInfo, ItemInfo, TechState, TechTarget, CraftOption types
│   │   └── tech_state.py                     # TechGraph & `infer_tech_state_from_world` / `suggest_next_targets`
│   ├── skills/                               # M5: Skill registry & implementations
│   │   ├── base/                             # Concrete base-level skill implementations
│   │   │   ├── chop_tree.py                  # Concrete chop_tree skill implementation
│   │   │   ├── feed_coke_ovens.py            # Concrete feed_coke_ovens implementation
│   │   │   └── __init__.py                   # Package marker
│   │   ├── __init__.py                       # Package marker; may expose registry helpers
│   │   ├── registry.py                       # Registers skills & maps names → callables
│   │   └── schema.py                         # Skill metadata, parameter schemas
│   ├── spec/                                 # M1: formal interfaces / shared spec types
│   │   ├── agent_loop.py                     # Spec types for agent loop interfaces
│   │   ├── bot_core.py                       # Spec for bot_core interface (actions, results)
│   │   ├── experience.py                     # Spec for learning/experience structures
│   │   ├── __init__.py                       # Package marker
│   │   ├── llm.py                            # Spec types for LLM requests/responses
│   │   ├── skills.py                         # Spec for skill definitions and calls
│   │   └── types.py                          # Core shared types (WorldState, Action, Observation, etc.)
│   └── virtues/                              # M4: Virtue lattice & metrics
│       ├── explain.py                        # Human-readable explanations for PlanScore (for logs/UI)
│       ├── features.py                       # Map plan + semantics + skills → PlanSummary feature vector
│       ├── __init__.py                       # Package marker; may expose public scoring API
│       ├── lattice.py                        # Core virtue lattice operations & scoring (score_plan, compare_plans)
│       ├── loader.py                         # Load virtues.yaml into in-memory structures
│       ├── metrics.py                        # (Legacy/extra) metric helpers if needed; can be folded into features
│       ├── sanity.py                         # Config sanity checks for virtues.yaml
│       └── schema.py                         # Virtue node/context/derived virtue / PlanSummary / PlanScore dataclasses
└── tests/                                    # Full unit/integration test suite
    ├── conftest.py                           # Shared fixtures & setup
    ├── fakes/                                # Fake implementations for isolated tests
    │   ├── fake_bot_core.py                  # Fake bot core (no Minecraft) for agent loop tests
    │   ├── fake_llm_stack.py                 # Deterministic fake LLMStack backend
    │   ├── fake_skills.py                    # Simple fake skills for planning/execution tests
    │   └── __init__.py                       # Package marker
    ├── __init__.py                           # Package marker
    ├── test_agent_loop_v1.py                 # Agent loop integration tests
    ├── test_architecture_integration.py      # Sanity: wiring holds together across major modules
    ├── test_env_loader.py                    # env.loader, YAML configs, and EnvProfile tests
    ├── test_error_model_with_fake_backend.py # ErrorModel behavior with fake backend
    ├── test_llm_stack_fake_backend.py        # LLMStack orchestration w/ fake backend
    ├── test_observation_critic_encoding.py   # Observation → critic encoding schema/path
    ├── test_observation_planner_encoding.py  # Observation → planner encoding schema/path
    ├── test_observation_worldstate_normalization.py # Ensures encoder builds normalized WorldState
    ├── test_phase0_runtime.py                # Phase 0 integration smoke tests (env + runtime wiring)
    ├── test_scribe_model_with_fake_backend.py# Scribe summarization behavior
    ├── test_semantics_caching_singleton.py   # SemanticsDB/TechGraph singleton caching semantics
    ├── test_semantics_categorization.py      # Block/item category & material inference
    ├── test_semantics_craftability.py        # craftable_items behavior & constraints
    ├── test_semantics_tech_inference.py      # infer_tech_state & suggest_next_targets behavior
    ├── test_semantics_tolerant_fallbacks.py  # Legacy / sloppy input tolerance in semantics
    ├── test_semantics_with_normalized_worldstate.py # End-to-end semantics using encoder-built WorldState
    ├── test_virtue_compare_plans.py          # compare_plans prefers more virtuous plan in context
    ├── test_virtue_config_sanity.py          # virtues.yaml sanity validation tests
    ├── test_virtue_hard_constraints.py       # Placeholder for constraint-based plan rejection tests
    └── test_virtue_lattice_basic.py          # Basic lattice scoring behavior tests

```

## GTNH_Agent – Updated Knowledge Capsule (with 7-Qualities Integration)

### 1. Context: Where we are

- Phase P0 done:
    
    - M0 environment foundation
        
    - M1 architecture spec
        
- Phase P1 in progress:
    
    - M2 basic LLM stack skeleton
        
    - M3 semantics shell
        
    - **M4 virtue_lattice: fully implemented & tested**
        

So far the mental model was “walk modules in numeric order.”  
Your new doc says: actually, **the real backbone is 7 _qualities_ that cut across modules**, and the build order should gate around those, not the raw module numbers.

You basically turned the project into FTB Interactions:  
little progress in multiple trees to unlock the big stuff without dumb backtracking.

---

### 2. The 7 Essential Qualities (and where they live)

These are the _behaviors_ the agent must have, mapped onto modules:

1. **Self-evaluation + retry loop**
    
    - Primary: **M8 agent_loop**
        
    - Support: M2 (Critic + ErrorModel), M9 (monitoring)
        
    - Core loop:  
        `observe → propose_plan → score with virtues → maybe_retry → execute → postmortem → summarize`
        
2. **Dynamic skill evolution & versioning**
    
    - Primary: **M5 skill_registry**
        
    - Extended by: M10 skill_learning
        
    - Needs:
        
        - versioned skills (`version`, `status`, `origin`, metrics)
            
        - registry functions for promotion / deprecation
            
3. **Hierarchical planning**
    
    - Primary: **M8 agent_loop**
        
    - Support: M2 (planner modes), spec layer (`AgentGoal`, `TaskPlan`, `SkillInvocation`)
        
    - Levels: Goal → Task → Skill → Action
        
4. **Experience memory**
    
    - Primary: **M10 skill_learning**
        
    - Hooks: M8 (episode trace), M2.scribe, M9
        
    - Experience schema: problem, goal, plan, attempts, outcome, virtue_scores, lessons
        
5. **Curriculum-driven goal selection**
    
    - Primary: **M11 curriculum engine**
        
    - Integrated with: M3 (TechState), M4 (virtue contexts), M8 (loop)
        
    - `curriculum.next_goal(tech_state)` becomes the top of the loop
        
6. **Structured LLM-role separation**
    
    - Primary: **M2 llm_stack_local**
        
    - Config: `config/llm_roles.yaml`
        
    - Each role has: system prompt, params, schema, permissions
        
    - Stack exposes: `call_planner`, `call_critic`, `call_scribe`, `call_error_model`
        
7. **Lightweight predictive world-model**
    
    - Primary: **M3 world_semantics_gtnh** (new `world_model.py`)
        
    - Used by: M4, M8, M11
        
    - Functions like:
        
        - `simulate_tech_progress(...)`
            
        - `estimate_infra_effect(...)`
            
        - `estimate_resource_trajectory(...)`
            

So instead of “M3, then M4, then M5…”, you now think in terms of:

> “Does the agent have: self-eval, skill evolution, hierarchy, memory, curriculum, role separation, prediction?”

Modules are just where those qualities physically sit.

---

### 3. New Recommended Implementation Order

Old idea: strict module chain.  
New idea: **gated progression**:

1. **Finish M4 + M5**
    
    - Moral spine (M4) ✅
        
    - Next: skill_registry with versioning hooks (M5)
        
2. **Complete M6 (functionally)**
    
    - Bot body / integration enough to move & act, even if jank
        
3. **Complete M7**
    
    - Observation encoding / mapping from bot snapshots → semantic state
        
4. **Prototype M8 loop**
    
    - Basic observe → plan → act → log loop, no fancy hierarchy yet
        
5. **Add LLM-role separation (M2 polish)**
    
    - Make planner / critic / scribe / error_model cleanly separated
        
6. **Add self-evaluation loop (M8 + M2)**
    
    - Reflexion-style retry using critic + virtue lattice
        
7. **Add hierarchical planning (M8 + M2)**
    
    - Goals → tasks → skills
        
    - Use specs to formalize
        
8. **Add experience memory (M8 + M10)**
    
    - Log episodes into replay buffer
        
    - Add retrieval hooks
        
9. **Add predictive world-model (M3)**
    
    - Minimal simulation / forecasting functions
        
    - Feed into M4 + M8 for “reject obviously stupid plan” behavior
        
10. **Add curriculum engine (M11)**
    

- `next_goal(tech_state, memory, virtue_context)`
    
- Controls which context_id M4 runs in
    

11. **Add full skill evolution (M5 + M10)**
    

- Promotion/demotion of synthesized skills
    
- Candidate → active → deprecated lifecycle
    

This is your FTB Interactions pattern:

- You don’t max out one tab before touching others.
    
- You unlock the minimal pieces of several modules to enable a quality, then push forward.
    

---

### 4. How this changes how we think about modules

Before:

- “M4 is the virtue module; later we’ll do learning, planning, etc.”
    

Now:

- **M4 is the value manifold** everything else orbits.
    
- M5 / M10 define “how skills evolve inside that value space.”
    
- M8 is the “nervous system” wiring loops that repeatedly query values (M4), goals (M11), and models (M3).
    
- M2 is the “voice in the head” but _tightly caged_ by roles + schemas.
    
- M3 stops being just “ontology” and becomes a **predictive world-model** hook.
    

So future design decisions shouldn’t ask:

> “What goes in M8 vs M10 vs M11?”

They should ask:

> “What quality am I trying to realize, and which module owns that piece of it?”

---

### 5. Concrete To-Do List (Next Steps)

Short version:

1. **M5: Skill registry with versioning fields**
    
    - `version`, `status`, `origin`, `metrics`
        
    - helper functions: `get_latest_skill`, `register_skill_candidate`, `mark_deprecated`
        
2. **M2: llm_roles.yaml + role-specific wrappers**
    
    - Basic role separation so planner / critic / scribe are cleanly distinct
        
3. **M8: minimal agent_loop prototype**
    
    - No hierarchy yet
        
    - But must:
        
        - generate candidate plans
            
        - call semantics (stub)
            
        - call M4 to pick best
            
        - log result
            
4. **M10: define Experience schema**
    
    - Even if not fully wired, lock the dataclass & storage format now so you don’t have to refactor later.
        
5. **M3: stub world_model.py**
    
    - Just signatures for now:
        
        - `simulate_tech_progress`
            
        - `estimate_infra_effect`
            
    - Adds a clear “plug here later” sign.
        

Once those are in place, you’ve basically laid down the rails for all 7 qualities, even if some are still unpowered.

---

### 6. How to remember this without thinking

Mental shortcut:

- **M4**: “How should I act?”
    
- **M2**: “What does the noisy oracle say?”
    
- **M3**: “What world am I in and what happens if I do X?”
    
- **M5 / M10**: “What can I do, and how is that evolving?”
    
- **M8**: “What do I actually do next?”
    
- **M9 / M11**: “What story does this behavior tell over time, and what are we aiming at?”
    

You’ve basically converted GTNH_Agent from “pile of modules” into “agent with seven clear psychological traits you’re engineering deliberately.”

Annoyingly competent move.